{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add50dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a062ea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_encoding(model_inputs, indent=4):\n",
    "    \"\"\"\n",
    "    Print the model inputs in a formatted way\n",
    "    \n",
    "    Args:\n",
    "        model_inputs (dict): Dictionary containing model inputs\n",
    "        indent (int, optional): Number of spaces for indentation. Defaults to 4.\n",
    "    \"\"\"\n",
    "    indent_str = \" \" * indent\n",
    "    print(\"{\")\n",
    "    for k, v in model_inputs.items():\n",
    "        # Print each key-value pair with proper indentation\n",
    "        print(indent_str + k + \":\")\n",
    "        print(indent_str + indent_str + str(v))\n",
    "    print(\"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75b6fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('siebert/sentiment-roberta-large-english')\n",
    "# Initialize the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained('siebert/sentiment-roberta-large-english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56c14b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    input_ids:\n",
      "        tensor([[    0,   100,   524,  2283,     7,   304, 30581,  3923, 12346,    18,\n",
      "          7891,   268,  5560,   328,    14,    18,  6344,     2]])\n",
      "    attention_mask:\n",
      "        tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "}\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-3.7820,  2.9508]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "tokenized input:  {'input_ids': tensor([[    0,   100,   524,  2283,     7,   304, 30581,  3923, 12346,    18,\n",
      "          7891,   268,  5560,   328,    14,    18,  6344,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Predicted label: Positive\n"
     ]
    }
   ],
   "source": [
    "input_text = \"I am excited to use Hugging Face's transformers library! that's awesome\"\n",
    "tokenized_input = tokenizer(input_text, return_tensors = 'pt')\n",
    "print_encoding(tokenized_input)\n",
    "output = model(**tokenized_input)\n",
    "print(output)\n",
    "\n",
    "labels = ['Negative','Positive']\n",
    "pred = torch.argmax(output.logits, dim = -1)\n",
    "\n",
    "print('tokenized input: ', tokenized_input)\n",
    "\n",
    "print(f'Predicted label: {labels[pred]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16b46fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertTokenizerFast(name_or_path='distilbert/distilbert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "name = \"distilbert/distilbert-base-cased\"\n",
    "\n",
    "\n",
    "# name = \"user/name\" when loading from\n",
    "# name = local_path when using save_pretrained() method\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(name) # convenient! Defaults to Fast\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d07ccc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla Tokenization\n",
      "{\n",
      "    input_ids:\n",
      "        [101, 146, 1567, 1106, 1505, 5428, 102]\n",
      "    attention_mask:\n",
      "        [1, 1, 1, 1, 1, 1, 1]\n",
      "}\n",
      "\n",
      "[101, 146, 1567, 1106, 1505, 5428, 102]\n",
      "[101, 146, 1567, 1106, 1505, 5428, 102]\n"
     ]
    }
   ],
   "source": [
    "# This is how you call the tokenizer\n",
    "input_str = \"I love to play cricket\"\n",
    "tokenized_inputs = tokenizer(input_str) # https://huggingface.co/learn/nlp-course/en/chapter6/6\n",
    "\n",
    "\n",
    "print(\"Vanilla Tokenization\")\n",
    "print_encoding(tokenized_inputs)\n",
    "print()\n",
    "\n",
    "# Two ways to access:\n",
    "print(tokenized_inputs.input_ids)\n",
    "print(tokenized_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1bb38779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 102 0\n",
      "['I', 'love', 'to', 'play', 'cricket']\n",
      "[146, 1567, 1106, 1505, 5428]\n",
      "[101, 146, 1567, 1106, 1505, 5428, 102]\n",
      "I love to play cricket\n",
      "start:                 I love to play cricket\n",
      "tokenize:              ['I', 'love', 'to', 'play', 'cricket']\n",
      "convert_tokens_to_ids: [146, 1567, 1106, 1505, 5428]\n",
      "add special tokens:    [101, 146, 1567, 1106, 1505, 5428, 102]\n",
      "--------\n",
      "decode:                I love to play cricket\n"
     ]
    }
   ],
   "source": [
    "cls = tokenizer.cls_token_id\n",
    "sep = tokenizer.sep_token_id\n",
    "pad = tokenizer.pad_token_id\n",
    "print(cls, sep, pad)\n",
    "\n",
    "#Tokenizer happen in a few steps\n",
    "input_token = tokenizer.tokenize(input_str)\n",
    "print(input_token)\n",
    "input_ids = tokenizer.convert_tokens_to_ids(input_token)\n",
    "print(input_ids)\n",
    "input_ids_special_tokens = [cls] + input_ids + [sep]\n",
    "print(input_ids_special_tokens)\n",
    "\n",
    "decoded_str = tokenizer.decode(input_ids_special_tokens, skip_special_tokens=True)\n",
    "print(decoded_str)\n",
    "\n",
    "print(\"start:                \", input_str)\n",
    "print(\"tokenize:             \", input_token)\n",
    "print(\"convert_tokens_to_ids:\", input_ids)\n",
    "print(\"add special tokens:   \", input_ids_special_tokens)\n",
    "print(\"--------\")\n",
    "print(\"decode:               \", decoded_str)\n",
    "\n",
    "# NOTE that these steps don't create the attention mask or add the special characters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
